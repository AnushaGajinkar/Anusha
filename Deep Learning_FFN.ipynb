{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset ,DataLoader\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                             transforms.Normalize([0.5],[0.5])])\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST('MNIST_data/', download=False, train=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=64\n",
    "train_loader=torch.utils.data.DataLoader(trainset,batch_size=batchsize,shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(testset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.sigmoid(self.fc1(x)))\n",
    "       \n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 64\n",
    "output_dim = 10\n",
    "learning_rate = 0.1\n",
    "model=Network(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000000000720D5E8>\n",
      "4\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n",
      "No.of parameters 50890\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())\n",
    "print(\"No.of parameters\", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.718..  Test Loss: 0.383..  Test Accuracy: 0.893\n",
      "Epoch: 2/10..  Training Loss: 0.364..  Test Loss: 0.293..  Test Accuracy: 0.915\n",
      "Epoch: 3/10..  Training Loss: 0.309..  Test Loss: 0.256..  Test Accuracy: 0.923\n",
      "Epoch: 4/10..  Training Loss: 0.275..  Test Loss: 0.230..  Test Accuracy: 0.932\n",
      "Epoch: 5/10..  Training Loss: 0.252..  Test Loss: 0.204..  Test Accuracy: 0.943\n",
      "Epoch: 6/10..  Training Loss: 0.233..  Test Loss: 0.185..  Test Accuracy: 0.946\n",
      "Epoch: 7/10..  Training Loss: 0.217..  Test Loss: 0.169..  Test Accuracy: 0.951\n",
      "Epoch: 8/10..  Training Loss: 0.205..  Test Loss: 0.159..  Test Accuracy: 0.953\n",
      "Epoch: 9/10..  Training Loss: 0.196..  Test Loss: 0.152..  Test Accuracy: 0.955\n",
      "Epoch: 10/10..  Training Loss: 0.186..  Test Loss: 0.140..  Test Accuracy: 0.960\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "                labels = Variable(labels)\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model B:  1 Hidden Layer Feedforward Neural Network (Tanh Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkT1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.tanh(self.fc1(x)))\n",
    "       \n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 64\n",
    "output_dim = 10\n",
    "learning_rate = 0.1\n",
    "model=NetworkT1(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x0000000007191048>\n",
      "4\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64])\n",
      "torch.Size([10, 64])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anusha\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.434..  Test Loss: 0.278..  Test Accuracy: 0.914\n",
      "Epoch: 2/10..  Training Loss: 0.260..  Test Loss: 0.194..  Test Accuracy: 0.943\n",
      "Epoch: 3/10..  Training Loss: 0.220..  Test Loss: 0.207..  Test Accuracy: 0.936\n",
      "Epoch: 4/10..  Training Loss: 0.197..  Test Loss: 0.159..  Test Accuracy: 0.952\n",
      "Epoch: 5/10..  Training Loss: 0.183..  Test Loss: 0.142..  Test Accuracy: 0.958\n",
      "Epoch: 6/10..  Training Loss: 0.171..  Test Loss: 0.120..  Test Accuracy: 0.965\n",
      "Epoch: 7/10..  Training Loss: 0.160..  Test Loss: 0.121..  Test Accuracy: 0.963\n",
      "Epoch: 8/10..  Training Loss: 0.153..  Test Loss: 0.107..  Test Accuracy: 0.968\n",
      "Epoch: 9/10..  Training Loss: 0.149..  Test Loss: 0.106..  Test Accuracy: 0.968\n",
      "Epoch: 10/10..  Training Loss: 0.143..  Test Loss: 0.095..  Test Accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "                labels = Variable(labels)\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelC: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkR1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "       \n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 64\n",
    "output_dim = 10\n",
    "learning_rate = 0.1\n",
    "model=NetworkR1(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.491..  Test Loss: 0.379..  Test Accuracy: 0.871\n",
      "Epoch: 2/10..  Training Loss: 0.275..  Test Loss: 0.248..  Test Accuracy: 0.917\n",
      "Epoch: 3/10..  Training Loss: 0.231..  Test Loss: 0.149..  Test Accuracy: 0.955\n",
      "Epoch: 4/10..  Training Loss: 0.204..  Test Loss: 0.133..  Test Accuracy: 0.959\n",
      "Epoch: 5/10..  Training Loss: 0.191..  Test Loss: 0.157..  Test Accuracy: 0.950\n",
      "Epoch: 6/10..  Training Loss: 0.178..  Test Loss: 0.109..  Test Accuracy: 0.967\n",
      "Epoch: 7/10..  Training Loss: 0.168..  Test Loss: 0.100..  Test Accuracy: 0.970\n",
      "Epoch: 8/10..  Training Loss: 0.161..  Test Loss: 0.100..  Test Accuracy: 0.970\n",
      "Epoch: 9/10..  Training Loss: 0.157..  Test Loss: 0.102..  Test Accuracy: 0.969\n",
      "Epoch: 10/10..  Training Loss: 0.151..  Test Loss: 0.099..  Test Accuracy: 0.969\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "                labels = Variable(labels)\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkR2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.1\n",
    "model=NetworkR2()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000000000721A930>\n",
      "6\n",
      "torch.Size([128, 784])\n",
      "torch.Size([128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 2.298..  Test Loss: 2.306..  Test Accuracy: 0.104\n",
      "Epoch: 2/10..  Training Loss: 2.296..  Test Loss: 2.304..  Test Accuracy: 0.099\n",
      "Epoch: 3/10..  Training Loss: 2.300..  Test Loss: 2.304..  Test Accuracy: 0.112\n",
      "Epoch: 4/10..  Training Loss: 2.296..  Test Loss: 2.306..  Test Accuracy: 0.112\n",
      "Epoch: 5/10..  Training Loss: 2.289..  Test Loss: 2.308..  Test Accuracy: 0.098\n",
      "Epoch: 6/10..  Training Loss: 2.293..  Test Loss: 2.303..  Test Accuracy: 0.112\n",
      "Epoch: 7/10..  Training Loss: 2.291..  Test Loss: 2.304.. "
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        #images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        #labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps=model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of parameters 109386\n"
     ]
    }
   ],
   "source": [
    "print(\"No.of parameters\", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkR3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.1\n",
    "model=NetworkR3()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No.of parameters\", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000000000720D6D8>\n",
      "8\n",
      "torch.Size([256, 784])\n",
      "torch.Size([256])\n",
      "torch.Size([128, 256])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.643..  Test Loss: 0.235..  Test Accuracy: 0.928\n",
      "Epoch: 2/10..  Training Loss: 0.251..  Test Loss: 0.168..  Test Accuracy: 0.949\n",
      "Epoch: 3/10..  Training Loss: 0.193..  Test Loss: 0.141..  Test Accuracy: 0.956\n",
      "Epoch: 4/10..  Training Loss: 0.161..  Test Loss: 0.109..  Test Accuracy: 0.966\n",
      "Epoch: 5/10..  Training Loss: 0.140..  Test Loss: 0.113..  Test Accuracy: 0.966\n",
      "Epoch: 6/10..  Training Loss: 0.125..  Test Loss: 0.096..  Test Accuracy: 0.969\n",
      "Epoch: 7/10..  Training Loss: 0.113..  Test Loss: 0.072..  Test Accuracy: 0.977\n",
      "Epoch: 8/10..  Training Loss: 0.108..  Test Loss: 0.056..  Test Accuracy: 0.983\n",
      "Epoch: 9/10..  Training Loss: 0.096..  Test Loss: 0.056..  Test Accuracy: 0.983\n",
      "Epoch: 10/10..  Training Loss: 0.092..  Test Loss: 0.048..  Test Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "                labels = Variable(labels)\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model F: 4 Hidden Layer Feedforward Neural Network (ReLU Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkR4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 384)\n",
    "        self.fc2 = nn.Linear(384, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 10)\n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc5(x), dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.1\n",
    "model=NetworkR4()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000000000720D5E8>\n",
      "10\n",
      "torch.Size([384, 784])\n",
      "torch.Size([384])\n",
      "torch.Size([256, 384])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.810..  Test Loss: 0.253..  Test Accuracy: 0.927\n",
      "Epoch: 2/10..  Training Loss: 0.269..  Test Loss: 0.166..  Test Accuracy: 0.949\n",
      "Epoch: 3/10..  Training Loss: 0.195..  Test Loss: 0.159..  Test Accuracy: 0.952\n",
      "Epoch: 4/10..  Training Loss: 0.163..  Test Loss: 0.150..  Test Accuracy: 0.954\n",
      "Epoch: 5/10..  Training Loss: 0.136..  Test Loss: 0.114..  Test Accuracy: 0.965\n",
      "Epoch: 6/10..  Training Loss: 0.122..  Test Loss: 0.089..  Test Accuracy: 0.972\n",
      "Epoch: 7/10..  Training Loss: 0.109..  Test Loss: 0.075..  Test Accuracy: 0.977\n",
      "Epoch: 8/10..  Training Loss: 0.100..  Test Loss: 0.054..  Test Accuracy: 0.984\n",
      "Epoch: 9/10..  Training Loss: 0.092..  Test Loss: 0.057..  Test Accuracy: 0.983\n",
      "Epoch: 10/10..  Training Loss: 0.087..  Test Loss: 0.053..  Test Accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "                labels = Variable(labels)\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of parameters 441802\n"
     ]
    }
   ],
   "source": [
    "print(\"No.of parameters\", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model G: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)-learning rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkR21(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.01\n",
    "model=NetworkR21()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000000000721A9A8>\n",
      "6\n",
      "torch.Size([128, 784])\n",
      "torch.Size([128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "print(len(list(model.parameters())))\n",
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.204..  Test Loss: 0.145..  Test Accuracy: 0.956\n",
      "Epoch: 2/10..  Training Loss: 0.192..  Test Loss: 0.135..  Test Accuracy: 0.959\n",
      "Epoch: 3/10..  Training Loss: 0.184..  Test Loss: 0.126..  Test Accuracy: 0.962\n",
      "Epoch: 4/10..  Training Loss: 0.175..  Test Loss: 0.118..  Test Accuracy: 0.964\n",
      "Epoch: 5/10..  Training Loss: 0.171..  Test Loss: 0.113..  Test Accuracy: 0.966\n",
      "Epoch: 6/10..  Training Loss: 0.162..  Test Loss: 0.107..  Test Accuracy: 0.968\n",
      "Epoch: 7/10..  Training Loss: 0.157..  Test Loss: 0.106..  Test Accuracy: 0.968\n",
      "Epoch: 8/10..  Training Loss: 0.151..  Test Loss: 0.104..  Test Accuracy: 0.969\n",
      "Epoch: 9/10..  Training Loss: 0.149..  Test Loss: 0.095..  Test Accuracy: 0.972\n",
      "Epoch: 10/10..  Training Loss: 0.143..  Test Loss: 0.090..  Test Accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        #images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        #labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps=model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of parameters 109386\n"
     ]
    }
   ],
   "source": [
    "print(\"No.of parameters\", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model H: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)-learning rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkR2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "model=NetworkR21()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.617..  Test Loss: 0.333..  Test Accuracy: 0.903\n",
      "Epoch: 2/10..  Training Loss: 0.294..  Test Loss: 0.202..  Test Accuracy: 0.940\n",
      "Epoch: 3/10..  Training Loss: 0.232..  Test Loss: 0.177..  Test Accuracy: 0.944\n",
      "Epoch: 4/10..  Training Loss: 0.198..  Test Loss: 0.142..  Test Accuracy: 0.957\n",
      "Epoch: 5/10..  Training Loss: 0.172..  Test Loss: 0.113..  Test Accuracy: 0.965\n",
      "Epoch: 6/10..  Training Loss: 0.155..  Test Loss: 0.125..  Test Accuracy: 0.959\n",
      "Epoch: 7/10..  Training Loss: 0.139..  Test Loss: 0.090..  Test Accuracy: 0.972\n",
      "Epoch: 8/10..  Training Loss: 0.129..  Test Loss: 0.080..  Test Accuracy: 0.975\n",
      "Epoch: 9/10..  Training Loss: 0.123..  Test Loss: 0.077..  Test Accuracy: 0.976\n",
      "Epoch: 10/10..  Training Loss: 0.115..  Test Loss: 0.066..  Test Accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        #images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        #labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps=model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of parameters 109386\n"
     ]
    }
   ],
   "source": [
    "print(\"No.of parameters\", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model I: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)-learning rate = 0.05, momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkR2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "model=NetworkR21()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.632..  Test Loss: 0.285..  Test Accuracy: 0.916\n",
      "Epoch: 2/10..  Training Loss: 0.446..  Test Loss: 0.288..  Test Accuracy: 0.920\n",
      "Epoch: 3/10..  Training Loss: 0.400..  Test Loss: 0.255..  Test Accuracy: 0.926\n",
      "Epoch: 4/10..  Training Loss: 0.374..  Test Loss: 0.291..  Test Accuracy: 0.911\n",
      "Epoch: 5/10..  Training Loss: 0.346..  Test Loss: 0.235..  Test Accuracy: 0.932\n",
      "Epoch: 6/10..  Training Loss: 0.327..  Test Loss: 0.228..  Test Accuracy: 0.935\n",
      "Epoch: 7/10..  Training Loss: 0.314..  Test Loss: 0.203..  Test Accuracy: 0.941\n",
      "Epoch: 8/10..  Training Loss: 0.306..  Test Loss: 0.225..  Test Accuracy: 0.936\n",
      "Epoch: 9/10..  Training Loss: 0.297..  Test Loss: 0.181..  Test Accuracy: 0.948\n",
      "Epoch: 10/10..  Training Loss: 0.290..  Test Loss: 0.157..  Test Accuracy: 0.953\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        #images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        #labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps=model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of parameters 109386\n"
     ]
    }
   ],
   "source": [
    "print(\"No.of parameters\", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model J: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)-learning rate = 0.1, momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkR2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "\n",
    "        # output so no dropout here\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "model=NetworkR21()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9) \n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 1.449..  Test Loss: 1.289..  Test Accuracy: 0.502\n",
      "Epoch: 2/10..  Training Loss: 1.745..  Test Loss: 1.454..  Test Accuracy: 0.440\n",
      "Epoch: 3/10..  Training Loss: 2.044..  Test Loss: 2.309..  Test Accuracy: 0.112\n",
      "Epoch: 4/10..  Training Loss: 2.269..  Test Loss: 2.305..  Test Accuracy: 0.112\n",
      "Epoch: 5/10..  Training Loss: 2.265..  Test Loss: 2.306..  Test Accuracy: 0.099\n",
      "Epoch: 6/10..  Training Loss: 2.298..  Test Loss: 2.304..  Test Accuracy: 0.112\n",
      "Epoch: 7/10..  Training Loss: 2.296..  Test Loss: 2.305..  Test Accuracy: 0.112\n",
      "Epoch: 8/10..  Training Loss: 2.296..  Test Loss: 2.305..  Test Accuracy: 0.099\n",
      "Epoch: 9/10..  Training Loss: 2.297..  Test Loss: 2.306..  Test Accuracy: 0.099\n",
      "Epoch: 10/10..  Training Loss: 2.297..  Test Loss: 2.305..  Test Accuracy: 0.104\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        #images = Variable(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        #labels = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps=model(images)\n",
    "        loss=criterion(log_ps,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss=0\n",
    "        accuracy=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images,labels in test_loader:\n",
    "                log_ps=model(images)\n",
    "                test_loss+=criterion(log_ps,labels)\n",
    "                ps=torch.exp(log_ps)\n",
    "                top_p,top_class=ps.topk(1,dim=1)\n",
    "                equals=top_class==labels.view(*top_class.shape)\n",
    "                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n",
    "        model.train()\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
